{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist=tf.keras.datasets.mnist\n",
    "(X_train_full,y_train_full),(X_test,y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  38,  48,  48,  22,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         62,  97, 198, 243, 254, 254, 212,  27,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  67,\n",
       "        172, 254, 254, 225, 218, 218, 237, 248,  40,   0,  21, 164, 187,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 219,\n",
       "        254,  97,  67,  14,   0,   0,  92, 231, 122,  23, 203, 236,  59,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  25, 217, 242,\n",
       "         92,   4,   0,   0,   0,   0,   4, 147, 253, 240, 232,  92,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 101, 255,  92,\n",
       "          0,   0,   0,   0,   0,   0, 105, 254, 254, 177,  11,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 167, 244,  41,\n",
       "          0,   0,   0,   7,  76, 199, 238, 239,  94,  10,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 192, 121,   0,\n",
       "          0,   2,  63, 180, 254, 233, 126,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 190, 196,  14,\n",
       "          2,  97, 254, 252, 146,  52,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 130, 225,  71,\n",
       "        180, 232, 181,  60,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 130, 254, 254,\n",
       "        230,  46,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   6,  77, 244, 254, 162,\n",
       "          4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 110, 254, 218, 254, 116,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 131, 254, 154,  28, 213,  86,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  66, 209, 153,  19,  19, 233,  60,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 142, 254, 165,   0,  14, 216, 167,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  90, 254, 175,   0,  18, 229,  92,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  26, 229, 249, 176, 222, 244,  44,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  73, 193, 197, 134,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full[59999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create validation from full training dataset\n",
    "#scale the data between 1-0 by dividing data by 255\n",
    "X_valid,X_train=X_train_full[:5000]/255,X_train_full[:5000]/255\n",
    "y_valid,y_train=y_train_full[:5000],y_train_full[:5000]\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS=[tf.keras.layers.Flatten(input_shape=[28,28],name='inputLayer')\n",
    "       ,tf.keras.layers.Dense(300,activation='relu',name='HiddenLayer1')\n",
    "        ,tf.keras.layers.Dense(300,activation='relu',name='HiddenLayer12')\n",
    "        ,tf.keras.layers.Dense(300,activation='relu',name='HiddenLayer13')\n",
    "        ,tf.keras.layers.Dense(300,activation='relu',name='HiddenLayer14')\n",
    "        ,tf.keras.layers.Dense(300,activation='relu',name='HiddenLayer15')\n",
    "        ,tf.keras.layers.Dense(10,activation='softmax',name='OutputLayer')\n",
    "       ]\n",
    "model_clf=tf.keras.models.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputLayer (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "HiddenLayer1 (Dense)         (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "HiddenLayer12 (Dense)        (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "HiddenLayer13 (Dense)        (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "HiddenLayer14 (Dense)        (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "HiddenLayer15 (Dense)        (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "OutputLayer (Dense)          (None, 10)                3010      \n",
      "=================================================================\n",
      "Total params: 599,710\n",
      "Trainable params: 599,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1=model_clf.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.02255782,  0.0432272 , -0.05359783, ...,  0.04975715,\n",
       "          0.01450276, -0.0010449 ],\n",
       "        [ 0.03194702,  0.03532905,  0.04906221, ..., -0.00732724,\n",
       "          0.06978905,  0.05658393],\n",
       "        [ 0.04966351,  0.03818464,  0.05245289, ...,  0.05722944,\n",
       "         -0.05221161, -0.0304397 ],\n",
       "        ...,\n",
       "        [-0.02284696, -0.06651   ,  0.04281913, ..., -0.02556128,\n",
       "         -0.02111065,  0.02434439],\n",
       "        [ 0.00494391,  0.02707141,  0.01622189, ...,  0.0040813 ,\n",
       "         -0.02550042, -0.03226038],\n",
       "        [ 0.04357383,  0.00346078, -0.03970285, ...,  0.05164349,\n",
       "         -0.03132434,  0.06683032]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FUNCTION='sparse_categorical_crossentropy'\n",
    "optimizer='SGD'\n",
    "metrics=['accuracy']\n",
    "model_clf.compile(loss=LOSS_FUNCTION,optimizer=optimizer,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 2.2201 - accuracy: 0.3500 - val_loss: 2.0677 - val_accuracy: 0.5492\n",
      "Epoch 2/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 1.6115 - accuracy: 0.6546 - val_loss: 1.0639 - val_accuracy: 0.7222\n",
      "Epoch 3/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.7786 - accuracy: 0.7944 - val_loss: 0.5928 - val_accuracy: 0.8350\n",
      "Epoch 4/50\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.5263 - accuracy: 0.8554 - val_loss: 0.4462 - val_accuracy: 0.8786\n",
      "Epoch 5/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.4138 - accuracy: 0.8850 - val_loss: 0.3957 - val_accuracy: 0.8838\n",
      "Epoch 6/50\n",
      "157/157 [==============================] - 2s 14ms/step - loss: 0.3517 - accuracy: 0.8988 - val_loss: 0.3139 - val_accuracy: 0.9154\n",
      "Epoch 7/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.3120 - accuracy: 0.9100 - val_loss: 0.3510 - val_accuracy: 0.8946\n",
      "Epoch 8/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.2792 - accuracy: 0.9208 - val_loss: 0.2443 - val_accuracy: 0.9326\n",
      "Epoch 9/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.2457 - accuracy: 0.9304 - val_loss: 0.2532 - val_accuracy: 0.9248\n",
      "Epoch 10/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.2247 - accuracy: 0.9374 - val_loss: 0.2268 - val_accuracy: 0.9320\n",
      "Epoch 11/50\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.2085 - accuracy: 0.9420 - val_loss: 0.2694 - val_accuracy: 0.9090\n",
      "Epoch 12/50\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.1926 - accuracy: 0.9414 - val_loss: 0.2279 - val_accuracy: 0.9288\n",
      "Epoch 13/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.1781 - accuracy: 0.9504 - val_loss: 0.1582 - val_accuracy: 0.9570\n",
      "Epoch 14/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.1627 - accuracy: 0.9510 - val_loss: 0.5049 - val_accuracy: 0.8332\n",
      "Epoch 15/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.1515 - accuracy: 0.9588 - val_loss: 0.1281 - val_accuracy: 0.9664\n",
      "Epoch 16/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.1358 - accuracy: 0.9632 - val_loss: 0.1307 - val_accuracy: 0.9668\n",
      "Epoch 17/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.1291 - accuracy: 0.9666 - val_loss: 0.2023 - val_accuracy: 0.9322\n",
      "Epoch 18/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.1165 - accuracy: 0.9684 - val_loss: 0.0961 - val_accuracy: 0.9774\n",
      "Epoch 19/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.1049 - accuracy: 0.9748 - val_loss: 0.0941 - val_accuracy: 0.9766\n",
      "Epoch 20/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0980 - accuracy: 0.9756 - val_loss: 0.1586 - val_accuracy: 0.9464\n",
      "Epoch 21/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0918 - accuracy: 0.9764 - val_loss: 0.0796 - val_accuracy: 0.9814\n",
      "Epoch 22/50\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.0810 - accuracy: 0.9818 - val_loss: 0.0658 - val_accuracy: 0.9848\n",
      "Epoch 23/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0753 - accuracy: 0.9818 - val_loss: 0.0787 - val_accuracy: 0.9790\n",
      "Epoch 24/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0682 - accuracy: 0.9840 - val_loss: 0.0610 - val_accuracy: 0.9856\n",
      "Epoch 25/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0627 - accuracy: 0.9846 - val_loss: 0.0519 - val_accuracy: 0.9904\n",
      "Epoch 26/50\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.0563 - accuracy: 0.9866 - val_loss: 0.0508 - val_accuracy: 0.9896\n",
      "Epoch 27/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0520 - accuracy: 0.9892 - val_loss: 0.0501 - val_accuracy: 0.9890\n",
      "Epoch 28/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0460 - accuracy: 0.9910 - val_loss: 0.0416 - val_accuracy: 0.9928\n",
      "Epoch 29/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0431 - accuracy: 0.9912 - val_loss: 0.0336 - val_accuracy: 0.9954\n",
      "Epoch 30/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0381 - accuracy: 0.9932 - val_loss: 0.0300 - val_accuracy: 0.9970\n",
      "Epoch 31/50\n",
      "157/157 [==============================] - 2s 11ms/step - loss: 0.0339 - accuracy: 0.9950 - val_loss: 0.0280 - val_accuracy: 0.9974\n",
      "Epoch 32/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0317 - accuracy: 0.9952 - val_loss: 0.0268 - val_accuracy: 0.9974\n",
      "Epoch 33/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0291 - accuracy: 0.9960 - val_loss: 0.0246 - val_accuracy: 0.9968\n",
      "Epoch 34/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0268 - accuracy: 0.9968 - val_loss: 0.0209 - val_accuracy: 0.9982\n",
      "Epoch 35/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0241 - accuracy: 0.9968 - val_loss: 0.0218 - val_accuracy: 0.9982\n",
      "Epoch 36/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0212 - accuracy: 0.9980 - val_loss: 0.0198 - val_accuracy: 0.9982\n",
      "Epoch 37/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0196 - accuracy: 0.9984 - val_loss: 0.0162 - val_accuracy: 0.9996\n",
      "Epoch 38/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0187 - accuracy: 0.9986 - val_loss: 0.0162 - val_accuracy: 0.9994\n",
      "Epoch 39/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0167 - accuracy: 0.9994 - val_loss: 0.0134 - val_accuracy: 0.9996\n",
      "Epoch 40/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0149 - accuracy: 0.9996 - val_loss: 0.0186 - val_accuracy: 0.9980\n",
      "Epoch 41/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0140 - accuracy: 0.9992 - val_loss: 0.0171 - val_accuracy: 0.9986\n",
      "Epoch 42/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0136 - accuracy: 0.9994 - val_loss: 0.0532 - val_accuracy: 0.9836\n",
      "Epoch 43/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0129 - accuracy: 0.9990 - val_loss: 0.0109 - val_accuracy: 0.9998\n",
      "Epoch 44/50\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0113 - accuracy: 0.9994 - val_loss: 0.0097 - val_accuracy: 0.9998\n",
      "Epoch 45/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0104 - accuracy: 0.9996 - val_loss: 0.0092 - val_accuracy: 0.9998\n",
      "Epoch 46/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0099 - accuracy: 0.9998 - val_loss: 0.0093 - val_accuracy: 0.9998\n",
      "Epoch 47/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0091 - accuracy: 0.9998 - val_loss: 0.0082 - val_accuracy: 0.9998\n",
      "Epoch 48/50\n",
      "157/157 [==============================] - 1s 9ms/step - loss: 0.0089 - accuracy: 0.9998 - val_loss: 0.0074 - val_accuracy: 0.9998\n",
      "Epoch 49/50\n",
      "157/157 [==============================] - 2s 10ms/step - loss: 0.0081 - accuracy: 0.9998 - val_loss: 0.0076 - val_accuracy: 0.9998\n",
      "Epoch 50/50\n",
      "157/157 [==============================] - 2s 12ms/step - loss: 0.0076 - accuracy: 0.9998 - val_loss: 0.0073 - val_accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "EPOCH=50\n",
    "VALIDATION_SET=(X_valid,y_valid)\n",
    "history=model_clf.fit(X_train,y_train,epochs=EPOCH,validation_data=VALIDATION_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your model will be stored at following location Saved Models\\Model_2021_01_07_21_39_21_.h5\n"
     ]
    }
   ],
   "source": [
    "def modelSave(model_dir=\"Saved Models\"):\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    file_name=time.strftime(\"Model_%Y_%m_%d_%H_%M_%S_.h5\")\n",
    "    model_path=os.path.join(model_dir,file_name)\n",
    "    print(f\"your model will be stored at following location {model_path}\")\n",
    "    return model_path\n",
    "UNIQUE_PATH=model_clf.save(modelSave())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.220104</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>2.067664</td>\n",
       "      <td>0.5492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.611483</td>\n",
       "      <td>0.6546</td>\n",
       "      <td>1.063859</td>\n",
       "      <td>0.7222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.778563</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.592779</td>\n",
       "      <td>0.8350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.526339</td>\n",
       "      <td>0.8554</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.8786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.413805</td>\n",
       "      <td>0.8850</td>\n",
       "      <td>0.395729</td>\n",
       "      <td>0.8838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.351730</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>0.313935</td>\n",
       "      <td>0.9154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.311961</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.351009</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.279248</td>\n",
       "      <td>0.9208</td>\n",
       "      <td>0.244253</td>\n",
       "      <td>0.9326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.245691</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>0.253246</td>\n",
       "      <td>0.9248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.224674</td>\n",
       "      <td>0.9374</td>\n",
       "      <td>0.226802</td>\n",
       "      <td>0.9320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.208482</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.269353</td>\n",
       "      <td>0.9090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.192648</td>\n",
       "      <td>0.9414</td>\n",
       "      <td>0.227950</td>\n",
       "      <td>0.9288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.178080</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.158158</td>\n",
       "      <td>0.9570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.162655</td>\n",
       "      <td>0.9510</td>\n",
       "      <td>0.504912</td>\n",
       "      <td>0.8332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.151528</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.9664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.135815</td>\n",
       "      <td>0.9632</td>\n",
       "      <td>0.130701</td>\n",
       "      <td>0.9668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.9666</td>\n",
       "      <td>0.202347</td>\n",
       "      <td>0.9322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.116513</td>\n",
       "      <td>0.9684</td>\n",
       "      <td>0.096092</td>\n",
       "      <td>0.9774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.104948</td>\n",
       "      <td>0.9748</td>\n",
       "      <td>0.094053</td>\n",
       "      <td>0.9766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.097982</td>\n",
       "      <td>0.9756</td>\n",
       "      <td>0.158582</td>\n",
       "      <td>0.9464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.091798</td>\n",
       "      <td>0.9764</td>\n",
       "      <td>0.079622</td>\n",
       "      <td>0.9814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.080983</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.065830</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.075349</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.078714</td>\n",
       "      <td>0.9790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.068204</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.061016</td>\n",
       "      <td>0.9856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.062717</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>0.051922</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.056288</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.050793</td>\n",
       "      <td>0.9896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.051972</td>\n",
       "      <td>0.9892</td>\n",
       "      <td>0.050106</td>\n",
       "      <td>0.9890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.046032</td>\n",
       "      <td>0.9910</td>\n",
       "      <td>0.041601</td>\n",
       "      <td>0.9928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.043085</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.9954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.038127</td>\n",
       "      <td>0.9932</td>\n",
       "      <td>0.029990</td>\n",
       "      <td>0.9970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.033886</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.027981</td>\n",
       "      <td>0.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>0.026818</td>\n",
       "      <td>0.9974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.029051</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.024578</td>\n",
       "      <td>0.9968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.026797</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.9982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.024142</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>0.021778</td>\n",
       "      <td>0.9982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.019751</td>\n",
       "      <td>0.9982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.019597</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>0.016248</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.018690</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.016708</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.013966</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.017126</td>\n",
       "      <td>0.9986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.013595</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.053225</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.012877</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.011263</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.010378</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.009310</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.007577</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.007268</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   2.220104    0.3500  2.067664        0.5492\n",
       "1   1.611483    0.6546  1.063859        0.7222\n",
       "2   0.778563    0.7944  0.592779        0.8350\n",
       "3   0.526339    0.8554  0.446154        0.8786\n",
       "4   0.413805    0.8850  0.395729        0.8838\n",
       "5   0.351730    0.8988  0.313935        0.9154\n",
       "6   0.311961    0.9100  0.351009        0.8946\n",
       "7   0.279248    0.9208  0.244253        0.9326\n",
       "8   0.245691    0.9304  0.253246        0.9248\n",
       "9   0.224674    0.9374  0.226802        0.9320\n",
       "10  0.208482    0.9420  0.269353        0.9090\n",
       "11  0.192648    0.9414  0.227950        0.9288\n",
       "12  0.178080    0.9504  0.158158        0.9570\n",
       "13  0.162655    0.9510  0.504912        0.8332\n",
       "14  0.151528    0.9588  0.128100        0.9664\n",
       "15  0.135815    0.9632  0.130701        0.9668\n",
       "16  0.129094    0.9666  0.202347        0.9322\n",
       "17  0.116513    0.9684  0.096092        0.9774\n",
       "18  0.104948    0.9748  0.094053        0.9766\n",
       "19  0.097982    0.9756  0.158582        0.9464\n",
       "20  0.091798    0.9764  0.079622        0.9814\n",
       "21  0.080983    0.9818  0.065830        0.9848\n",
       "22  0.075349    0.9818  0.078714        0.9790\n",
       "23  0.068204    0.9840  0.061016        0.9856\n",
       "24  0.062717    0.9846  0.051922        0.9904\n",
       "25  0.056288    0.9866  0.050793        0.9896\n",
       "26  0.051972    0.9892  0.050106        0.9890\n",
       "27  0.046032    0.9910  0.041601        0.9928\n",
       "28  0.043085    0.9912  0.033589        0.9954\n",
       "29  0.038127    0.9932  0.029990        0.9970\n",
       "30  0.033886    0.9950  0.027981        0.9974\n",
       "31  0.031700    0.9952  0.026818        0.9974\n",
       "32  0.029051    0.9960  0.024578        0.9968\n",
       "33  0.026797    0.9968  0.020921        0.9982\n",
       "34  0.024142    0.9968  0.021778        0.9982\n",
       "35  0.021161    0.9980  0.019751        0.9982\n",
       "36  0.019597    0.9984  0.016248        0.9996\n",
       "37  0.018690    0.9986  0.016216        0.9994\n",
       "38  0.016708    0.9994  0.013361        0.9996\n",
       "39  0.014925    0.9996  0.018584        0.9980\n",
       "40  0.013966    0.9992  0.017126        0.9986\n",
       "41  0.013595    0.9994  0.053225        0.9836\n",
       "42  0.012877    0.9990  0.010905        0.9998\n",
       "43  0.011263    0.9994  0.009745        0.9998\n",
       "44  0.010378    0.9996  0.009237        0.9998\n",
       "45  0.009913    0.9998  0.009310        0.9998\n",
       "46  0.009108    0.9998  0.008165        0.9998\n",
       "47  0.008869    0.9998  0.007432        0.9998\n",
       "48  0.008144    0.9998  0.007577        0.9998\n",
       "49  0.007644    0.9998  0.007268        0.9998"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2774 - accuracy: 0.9348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2774028778076172, 0.9348000288009644]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.99, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new=X_test[:2]\n",
    "y_prob=model_clf.predict(X_new)\n",
    "y_prob.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=np.argmax(model_clf.predict(X_new),axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_new=y_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM10lEQVR4nO3df6zd9V3H8eenP6StJcRCU4q0F0dRcF3pH2hXrbaJ6AABayuthtWSMc2IxvkjZmtXzSRNXaHINGZhicuYTFYGWlK7aGMTf21EsQrttGyMdWUF7xh0LRQEbtWvf3w/V749nPM5957b3vO+vc9HcpNzzvv7/Xw/3+85r+/n+yPn3FRVFZLimdLvDkhqz3BKQRlOKSjDKQVlOKWgDKcU1KQLZ0rpSErp2vx4c0rpT8ZhmatSSs+dhXYvSylVKaVpZ7rtc8FE3z6TLpxNVVVtq6rq/d2mSyndn1Laejb6kFL6SkrpfW1e/2BKaf/ZWOZopZTOSyl9KqX0bErpZErpyZTS9SOY77YcjvWjWFaVUlo0th6PeFk7Ukpfy+v0lZTSL47HckdqQodzou4RW3wGaPeh2JBrEUwDjgIrgQuALcDnU0qXdZlvI/Ad2q9fBK8BN1Gv00bgD1NKP9LfLjVUVRXqDzgCbAIOAceBTwMzcm0V8BzwIeBbwAPUO5gPA18HjgGfB+Y02tsAPJtrH8ntX5trHwU+25h2BfAYcIL6w3gb8MvAKWAIeBX4yzztJcCfAy8C3wB+rdHOTOD+3P9DwG8Dz3VY30uB/wYGGq/9YF7eRcBPA08Ar+Q+fbQx3WVABUxrbLtrG/XW9Xt3Y/0OAKvG8D4dBNYW6gPA/wJr8/pd3KhNBTbn9+wk8K/AAuAf8vq8lrf1+vwefLGl7QpYlB+PePuMYJ12A7/V7wz8f3/63YE2G+gI8O/5zZoDfAnYmmur8hu9HTgvh+CDwD/lD/l5wCeBzzU+5K8CP55rf5Dnf1s484fpJPALwHTgQmBprt0/3If8fEr+QP0u8F3AO4DDwHty/WPAP+b+L8jr0zacefq/AbY0nv8+8Ghjnd+Vl7kEeAFY3e7DRyGcwPdS76BuyG39ZH4+t0OfPgF8okNtHvAGcGVhnX4HeDw//nLzQ0+9s/oy8ANAAq4GLmwNXn5+G+Vwjmb7fBjY06G/M4FB4Lp+ZyB6OD/QeH4D8PXGGzFEHknza08BP9F4Pp96pJuWw7OzUfvuPH+7cG4CdnXo0/2cHs5lwDdbptkEfDo/Ptx8k6lH31I43wt8NT+eAnwT+NkO034cuLfDh+8IncP5IeCBlrb2AhtH+f5MB/YBn+wy3deAX29smwON2leBn+kw36jCOZrt06W/nwH+Gkj9zsDwX9RzzqONx89SH0IOe7GqqjcazweAXSmlEymlE9Rh/R/qvfslzbaqqnqNerRoZwH1YdZIDACXDC8zL3dzXiaty83rUPIXwPyU0rupd0CzgC8ApJSWpZT+NqX0YkrpZeAD1Ie7ozUA3NLS5xXUO7MRSSlNoT6VGAJ+tTDdjwLfB+zMLz0IvCultDQ/H8227tanMW+flNLdwGJgXZWTGkHUCyoLGo8XAv/ZeN668Y4C76uq6kutjaSUBoGrGs9nUR+utnMU+OEOtXbL/EZVVVd0mH6Qeh3+Iz9f2GG6uvGq+q+U0iPUF05mUo/2Q7n8IPDHwPVVVb2RUvo4nT98r1EHe9jFLX1+oKqqXyr1pZOUUgI+Rb0DuqGqqlOFyTdSH64+Wc922utP5r5cTn24381p65RSurilPprt8zYppd8DrgdWVlX1ykjnGxf9HrrbHF4coT4fuZT6nO2LwLZcW0XL4SHwG8DfkS+oAHPJh0zAO6nPOVdQnxvuoPM550Lqc8511Dut5jnnx4AHG8ucCvwb9aHizPx8MfBDub4d+Hvge/J6HGztd5v1Xkk9qr8y3E5+/dvkQ0/qnce3G32+jNMPa/+M+sM6HbgGeKkx7QLqi2jvyf2dkbfnpSN8X+6jPref3aFe5fZmUF9wup165zD89yvU54PTqM85DwJXUId4CW+dc34L+KlGu98PvAkszW3fx+nnnCPePm36vIn68PvikWyDcc9CvzvQIZzDV2tPUJ8LzCqEcwrwm9TnMSepD5e2Neobqc/hRnK19seAf+atK3/Db/oV1Hv8E7x1oeYS4HP5w3Q8f3CH250F/Gmevni1trHsRH2ueqjl9Z+jPiw+CeyhHiU6hfMduf+vUh8W/1HL+i2j3ml8h/oq8xeAhYUw3pcfD+TlvJHbHv67NdcX5G12IfDz1EcO01vam5nfgxupdw5bqK9ynwT+hbyToD4sHczbbl1+7SPUO5qj1OfnzXCOZvtsBv6q0aeKOvjNddrc7wwM/6XcyTBSSkeA91dVta/ffdHIpJTeC7yzqqpN/e7LuSTqOacmkKqqPtvvPpyLol6tlSa9cIe1kmqOnFJQ3c45HValsy+1e9GRUwrKcEpBGU4pKMMpBWU4paAMpxSU4ZSCMpxSUIZTCspwSkEZTikowykFZTiloAynFJThlIIynFJQhlMKynBKQRlOKSjDKQVlOKWgDKcUlP+O4SzYsWNHsf766693rB08eLA47yOPPNJTn4bdcccdxfry5cs71jZs2DCmZWt0HDmloAynFJThlIIynFJQhlMKynBKQRlOKahu/zzXfwHYxvr164v1hx9+eJx6cuYtWrSoY23fvn3FeRcuXHimuzNZ+C8ApYnEcEpBGU4pKMMpBWU4paAMpxSU4ZSC8j5nG/28j3nllVcW69ddd12xfvjw4WJ99+7do+7TsK1btxbrmzdv7rntSc77nNJEYjiloAynFJThlIIynFJQhlMKynBKQU3K363dv39/sb5r164xtb948eJivXSv8aKLLirOO3v27GJ9aGioWF+2bFmxfuDAgY61Y8eOFefVmeXIKQVlOKWgDKcUlOGUgjKcUlCGUwpqUt5KGRwcLNa7fI2u662SvXv3Fuvz588v1sei278ffOqpp3pu+8Ybb+x5Xo2eI6cUlOGUgjKcUlCGUwrKcEpBGU4pKMMpBTUp73PedNNNxfozzzxTrJ9//vnF+pw5c0bdpzPloYceKta7faVMcThySkEZTikowykFZTiloAynFJThlIIynFJQk/I+ZzcDAwP97kJHd999d7H+9NNPj6n90k9ndvtZTZ1ZjpxSUIZTCspwSkEZTikowykFZTiloAynFFTq8hut5R9w1Rm3Z8+eYv2WW24p1t98881ifd68ecX6zp07O9ZWrlxZnFc9S+1edOSUgjKcUlCGUwrKcEpBGU4pKMMpBWU4paD8Pmcw+/fvL9a73cfsZv369cW69zLjcOSUgjKcUlCGUwrKcEpBGU4pKMMpBeWtlD5YvXp1x9revXvH1PbGjRuL9a1bt46pfY0fR04pKMMpBWU4paAMpxSU4ZSCMpxSUIZTCsqfxjwLBgcHi/Wrr766Y+2ll14qzjt37txi/bHHHivWL7/88mJdfeFPY0oTieGUgjKcUlCGUwrKcEpBGU4pKMMpBeX3Oc+CNWvWFOvd7mWW3HrrrcW69zHPHY6cUlCGUwrKcEpBGU4pKMMpBWU4paAMpxSU9zl7sHv37mL9iSee6LntVatWFet33nlnz21rYnHklIIynFJQhlMKynBKQRlOKSjDKQVlOKWgvM/ZxrFjx4r1bdu2FetDQ0M9L3vp0qXF+uzZs3tuWxOLI6cUlOGUgjKcUlCGUwrKcEpBGU4pKG+ltHHPPfcU648//viY2l+9enXHml8J0zBHTikowykFZTiloAynFJThlIIynFJQhlMKKlVVVaoXi+eqGTNmFOtj+UoYwPPPP9+xNn/+/DG1rQkptXvRkVMKynBKQRlOKSjDKQVlOKWgDKcUlOGUgvL7nH1Q+unN6dOnj2NP3u6CCy7oWOvWt1OnThXrL7/8ck99Ajh+/Hixfu+99/bc9khMnTq1Y2379u3FeWfNmtXTMh05paAMpxSU4ZSCMpxSUIZTCspwSkEZTiko73P2wZIlS/rdhY7WrVvXsdbtu6YvvPBCsb5z586e+hTdvHnzivUtW7b01K4jpxSU4ZSCMpxSUIZTCspwSkEZTikofxqzjTVr1hTrjz766Ph0ZJIpfSVtypSxjSM333xzsX7NNdf03PaKFSuK9eXLl3drwp/GlCYSwykFZTiloAynFJThlIIynFJQhlMKyvucPbjrrruK9bH+i8CSQ4cOFetn82tZt99+e7E+MDAwpvbXrl3bsXbVVVeNqe3gvM8pTSSGUwrKcEpBGU4pKMMpBWU4paAMpxSU9zml/vM+pzSRGE4pKMMpBWU4paAMpxSU4ZSCMpxSUIZTCspwSkEZTikowykFZTiloAynFJThlIIynFJQhlMKynBKQRlOKSjDKQVlOKWgDKcUlOGUgjKcUlCGUwrKcEpBGU4pKMMpBWU4paAMpxSU4ZSCMpxSUIZTCspwSkEZTikowykFZTiloAynFJThlIKa1qWexqUXkt7GkVMKynBKQRlOKSjDKQVlOKWgDKcU1P8B6U8Xbx+GiRAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data,pred,actual in zip(X_new,y_pred,y_test_new):\n",
    "    plt.imshow(data,cmap=\"binary\")\n",
    "    plt.title(f\"predicted Value :{pred},Actual:{actual}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing my images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1; however, version 20.3.3 is available."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.1.48-cp38-cp38-win_amd64.whl (34.9 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\virat.singh\\anaconda3\\envs\\tf2\\lib\\site-packages (from opencv-python) (1.19.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "You should consider upgrading via the 'c:\\users\\virat.singh\\anaconda3\\envs\\tf2\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 3)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "img=mpimg.imread('six.jpg')\n",
    "im=cv2.resize(img,(28,28))\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2grey(rgb):\n",
    "    return np.dot(rgb[...,:3],[0.2989,0.5870,1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey=rgb2grey(im)\n",
    "grey=grey.reshape(1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(model_clf.predict(grey),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
